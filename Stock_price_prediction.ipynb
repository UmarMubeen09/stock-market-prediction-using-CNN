{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data into using Pandas Read_csv\n",
    "dataset_train = pd.read_csv(\"google-stock-price-data\\Google_Stock_Price_Train.csv\")\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets understand how Dataframe.iloc is working\n",
    "# Iloc representd for \"Index Location of\" \n",
    "# Try below commands removing comments\n",
    "#dataset_train.iloc[:]                       # This will give you all dataframe values\n",
    "#dataset_train.iloc[1]                        # this will return you your first index data#\n",
    "#dataset_train.iloc[:,0]                    # return all data of om index column\n",
    "#type(dataset_train.iloc[:,1])           # will return you all data of column 1 But output will be in Pandas.core.series.Series\n",
    "#dataset_train.iloc[:,1:2]             # Prefered  # will return all data of column 1 But output will be in Pandas Dataframe\n",
    "\n",
    "# Get your input data in variable (Open column data is our input data for our model)\n",
    "trainset = dataset_train.iloc[:,1:2].values        # .values return array \n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will scale out trainset AKA input value within scale of 0 to 1 \n",
    "# Min Max Scaler is precedure to Scale original value within maped value without loosing original information\n",
    "# This is Normalization technique\n",
    "#Normalization is used to scale the data between 0 and 1. It is defined as\n",
    "#Yi = [Xi - min(X)]/[max(X) - min(X)]\n",
    "#Where Xi is the i th data point and min represents the minimum and Maximum represents maximum. So Xi converts to Yi\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0,1))        # Feature_range you can define for your output range (default (0,1))\n",
    "training_set_scaled = sc.fit_transform(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scaled                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create our training and testing dataset By taking first 60 days data to predict 61st days stock price\n",
    "#### split data in x_train and y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will update our dataset in a way that our neural network will learn on basis of first two month data(60 days) to \n",
    "# predict next day price \n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(60,1258):                             # tere are total 1258 data \n",
    "    x_train.append(training_set_scaled[i-60:i,0])     # append first 60 data to x_train dataset \n",
    "    y_train.append(training_set_scaled[i,0])         #for first 60 x_train data 61st day data will be our y_train data  \n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train  = np.array(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape           # our Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape          # our y training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))      #reshape your dataset with adding one extra dimension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Creation using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Sequential()            # initiate Keras Sequential model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50,return_sequences = True,input_shape = (x_train.shape[1],1)))  #first layer input shape required \n",
    "regressor.add(Dropout(0.2))                               # setting dropout at each layer \n",
    "regressor.add(LSTM(units = 50,return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units = 50,return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(Dense(units = 1))                             # output layer with one node for prediction\n",
    "regressor.compile(optimizer = 'adam',loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now our model is set now we need to our dataset\n",
    "# fitting model with 100 epoch and batch size 32\n",
    "# here we have given x_train and y_train , our model will first predict output from input dataset at each stage and again\n",
    "# checked with y_label output then error will be calculated and furthaer weights assigned to nodes will be updated\n",
    "regressor.fit(x_train,y_train,epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test data \n",
    "dataset_test =pd.read_csv(\"google-stock-price-data\\Google_Stock_Price_Test.csv\")\n",
    "real_stock_price = dataset_test.iloc[:,1:2].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding both test data and train data in a single dataframe for \"Open\" column\n",
    "dataset_total = pd.concat((dataset_train['Open'],dataset_test['Open']),axis = 0)\n",
    "dataset_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input set for modele evaluation \n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test)-60:].values\n",
    "\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs.shape                      # there are totale 80 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sc.transform(inputs)     # Scale input values \n",
    "inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create x_test dataset as we have done earlier with 60 days grouped data \n",
    "\n",
    "x_test = []\n",
    "for i in range(60,80):\n",
    "    x_test.append(inputs[i-60:i,0])\n",
    "    \n",
    "x_test = np.array(x_test)\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Reshape input dataset\n",
    "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now predict stock price using predict method and passing x_test data\n",
    "predicted_price = regressor.predict(x_test)\n",
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price = sc.inverse_transform(predicted_price)   # get inverse_transformed value to map with original output\n",
    "predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price = predicted_price\n",
    "plt.plot(real_stock_price,color = 'red', label = 'Real Price')\n",
    "plt.plot(predicted_price, color = 'blue', label = 'Predicted Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see our Model is able to Recognise Upwards and Downwards of Prices \n",
    "## Our Blue line is able to Track than when price will go Up and when it will come down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
